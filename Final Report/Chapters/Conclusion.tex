\chapter{Conclusion and Future Work}

This research aimed to explore the potential of implementing a Naive Bayes Classifier into a minimax based chess engine to improve the evaluation. It investigated if a simpler machine learning algorithm could reduce complexity but achieve the performance of more complex machine learning techniques. Two implementations were discussed and evaluated in this report. The first implementation was MMNB substitution where the evaluation function for the minimax algorithm was replaced by the probability outputs of the Naive Bayes Classifier. The second implementation was MMNB integration where the state of a board was evaluated using both the Naive Bayes Classifier and a traditional evaluation function. 

The results of the experiments showed that MMNB integration outperformed MMNB substitution in overall win success but also other metrics like piece balance and time taken to make a move. It consistently did better in terms of effectiveness and efficiency. This reinforced the idea that the Naive Bayes Classifier when used standalone is not reliable but when used as a support for traditional evaluation functions, it can become more powerful.

The experiments also confirmed the importance of feature engineering. The results showed that the increase in features and complexity generally led to improved performance. This was shown by the fact that the best-performing feature set was feature set 2, which was able to balance between the complexity of the features and the performance of the Naive Bayes Classifier. However, it also helped discover that increasing the number of features does not always lead to better performance. Feature set 3 consistently underperformed the other feature sets, despite having the most features and most complexity, indicating the importance of carefully selecting the features. 

The influence of dataset choice was also discussed. The results indicated minimal impact of the level of games used to train the model. However, it did indicate that there was a slight advantage to the engines that were trained on the random sampled dataset, indicating a stronger ability to generalise to different positions. This suggests the importance of using a diverse dataset to be able to perform better against opponents of varied skill levels.  

Several limitations were also identified with using Naive Bayes. The primary limitation as discussed throughout the research is the assumption of independence between features. This is a very detrimental assumption to make in the context of chess where each component of the game is interdependent. This was evident by the poor performance of the engines against stockfish. Another limitation is the depth used in the minimax algorithm. Throughout this research, a depth of 3 was used as a compromise between performance and the time taken to make a move. The potential of Naive Bayes could be further explored by increasing the depth of the minimax algorithm.

This research has proven that there is some potential for the use of Naive Bayes in chess engines. More exploration could be done in the field to investigate in what use cases it could prove beneficial and effective. One major field of inquiry could be the dataset used to train the model. What could be interesting to explore is the impact of training the model on different phases of the game. Training models on only opening, midgame and endgame phases could provide interesting insights into at what stage of the game the Naive Bayes Classifier is most effective. 

In this research Naive Bayes was used alone, however another research domain would be to study the impact of using Naive Bayes alongside other machine learning techniques, like Neural Networks, which could result in an engine that benefits from the strengths of both techniques.

Feature sets were investigated in this research but the impact of each individual feature was not studied. This could be an interesting point to investigate, which features had teh most impact and if the complex features could be effective alone.


% The project's conclusions should list the key things that have been learnt as a consequence of engaging in your project work. For example, ``The use of overloading in C++ provides a very elegant mechanism for transparent parallelisation of sequential programs'', or ``The overheads of linear-time n-body algorithms makes them computationally less efficient than $O(n \log n) investigating systems with less than 100000 particles''. Avoid tedious personal reflections like ``I learned a lot about C++ programming...'', or ``Simulating colliding galaxies can be real fun...''. It is common to finish the report by listing ways in which the project can be taken further. This might, for example, be a plan for turning a piece of software or hardware into a marketable product, or a set of ideas for possibly turning your project into an MPhil or PhD.



% ???INCLUDE Training models on only a certain game phase like opening, mid or end

% Also investigating which features had the most impact, and if the complex features can be useful alone 

% ???