@inproceedings{davidDeepChessEndtoEndDeep2016,
  title = {{{DeepChess}}: {{End-to-End Deep Neural Network}} for {{Automatic Learning}} in {{Chess}}},
  shorttitle = {{{DeepChess}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} â€“ {{ICANN}} 2016},
  author = {David, Omid E. and Netanyahu, Nathan S. and Wolf, Lior},
  editor = {Villa, Alessandro E.P. and Masulli, Paolo and Pons Rivero, Antonio Javier},
  date = {2016},
  pages = {88--96},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-44781-0_11},
  abstract = {We present an end-to-end learning method for chess, relying on deep neural networks. Without any a priori knowledge, in particular without any knowledge regarding the rules of chess, a deep neural network is trained using a combination of unsupervised pretraining and supervised training. The unsupervised training extracts high level features from a given position, and the supervised training learns to compare two chess positions and select the more favorable one. The training relies entirely on datasets of several million chess games, and no further domain specific knowledge is incorporated.},
  isbn = {978-3-319-44781-0},
  langid = {english},
  keywords = {Chess Game,Deep Belief Network,Deep Neural Network,High Level Feature,Supervise Training},
  file = {C:\Users\ibyad\Zotero\storage\W48S2T4Q\David et al. - 2016 - DeepChess End-to-End Deep Neural Network for Automatic Learning in Chess.pdf}
}

@article{ghodasaraOverviewDecisionTree2021,
  title = {Overview of {{Decision Tree Pruning}} in {{Machine Learning}}},
  author = {Ghodasara, Kavisha},
  date = {2021},
  volume = {08},
  number = {08},
  abstract = {This document serves as an introduction to the Pruning of Decision Trees. Pruning, which serves to find a sparse subnetwork in a dense network that has the same overall accuracy, helps in reducing the space requirements and cost in operating the network. An introduction to the different approaches to pruning, types of pruning has been mentioned. The Lottery Ticket Hypothesis [1] has been mentioned in order to support the advantages of pruning, along with the strategies employed to do so. Alpha-beta Pruning, which often finds its use in multiplayer gaming to determine the next best moves to be made by a machine, has also been discussed. The merits and demerits of pruning so mentioned help in determining whether pruning would be a feasible option or not.},
  langid = {english},
  file = {C:\Users\ibyad\Zotero\storage\VQ4FMAGC\Ghodasara - 2021 - Overview of Decision Tree Pruning in Machine Learning.pdf}
}

@inproceedings{lowdNaiveBayesModels2005,
  title = {Naive {{Bayes}} Models for Probability Estimation},
  author = {Lowd, Daniel and Domingos, Pedro},
  date = {2005-01},
  doi = {10.1145/1102351.1102418},
  url = {https://www.researchgate.net/publication/221346504_Naive_Bayes_models_for_probability_estimation},
  urldate = {2024-12-11},
  abstract = {PDF | Naive Bayes models have been widely used for clustering and classification. However, they are seldom used for general probabilistic learning and... | Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  file = {C:\Users\ibyad\Zotero\storage\Z9RG42FZ\221346504_Naive_Bayes_models_for_probability_estimation.html}
}

@book{newbornKasparovDeepBlue1997,
  title = {Kasparov versus {{Deep Blue}}},
  author = {Newborn, Monty},
  date = {1997},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-2260-6},
  url = {http://link.springer.com/10.1007/978-1-4612-2260-6},
  urldate = {2024-12-10},
  isbn = {978-1-4612-7477-3 978-1-4612-2260-6},
  langid = {english},
  keywords = {chess,computer chess,computer chess championship,Deep Blue,Kasparov}
}

@online{NLPinChess,
  title = {{{SentiMATE}}: {{Learning}} to Play {{Chess}} through {{Natural Language Processing}}},
  shorttitle = {{{SentiMATE}}},
  author = {Kamlish, Isaac and Chocron, Isaac Bentata and McCarthy, Nicholas},
  date = {2019-09-25},
  eprint = {1907.08321},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1907.08321},
  url = {http://arxiv.org/abs/1907.08321},
  urldate = {2024-12-10},
  abstract = {We present SentiMATE, a novel end-to-end Deep Learning model for Chess, employing Natural Language Processing that aims to learn an effective evaluation function assessing move quality. This function is pre-trained on the sentiment of commentary associated with the training moves and is used to guide and optimize the agent's game-playing decision making. The contributions of this research are three-fold: we build and put forward both a classifier which extracts commentary describing the quality of Chess moves in vast commentary datasets, and a Sentiment Analysis model trained on Chess commentary to accurately predict the quality of said moves, to then use those predictions to evaluate the optimal next move of a Chess agent. Both classifiers achieve over 90 \% classification accuracy. Lastly, we present a Chess engine, SentiMATE, which evaluates Chess moves based on a pre-trained sentiment evaluation function. Our results exhibit strong evidence to support our initial hypothesis - "Can Natural Language Processing be used to train a novel and sample efficient evaluation function in Chess Engines?" - as we integrate our evaluation function into modern Chess engines and play against agents with traditional Chess move evaluation functions, beating both random agents and a DeepChess implementation at a level-one search depth - representing the number of moves a traditional Chess agent (employing the alpha-beta search algorithm) looks ahead in order to evaluate a given chess state.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\ibyad\\Zotero\\storage\\MITFITBV\\Kamlish et al. - 2019 - SentiMATE Learning to play Chess through Natural Language Processing.pdf;C\:\\Users\\ibyad\\Zotero\\storage\\HTDHSB4P\\1907.html}
}

@online{plaatResearchReSearch2024,
  title = {Research {{Re}}: Search \& {{Re-search}}},
  shorttitle = {Research {{Re}}},
  author = {Plaat, Aske},
  date = {2024-03-20},
  eprint = {2403.13705},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.13705},
  url = {http://arxiv.org/abs/2403.13705},
  urldate = {2024-12-10},
  abstract = {Search algorithms are often categorized by their node expansion strategy. One option is the depth-first strategy, a simple backtracking strategy that traverses the search space in the order in which successor nodes are generated. An alternative is the best-first strategy, which was designed to make it possible to use domain-specific heuristic information. By exploring promising parts of the search space first, best-first algorithms are usually more efficient than depth-first algorithms. In programs that play minimax games such as chess and checkers, the efficiency of the search is of crucial importance. Given the success of best-first algorithms in other domains, one would expect them to be used for minimax games too. However, all high-performance game-playing programs are based on a depth-first algorithm. This study takes a closer look at a depth-first algorithm, AB, and a best-first algorithm, SSS. The prevailing opinion on these algorithms is that SSS offers the potential for a more efficient search, but that its complicated formulation and exponential memory requirements render it impractical. The theoretical part of this work shows that there is a surprisingly straightforward link between the two algorithms -- for all practical purposes, SSS is a special case of AB. Subsequent empirical evidence proves the prevailing opinion on SSS to be wrong: it is not a complicated algorithm, it does not need too much memory, and it is also not more efficient than depth-first search.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\ibyad\\Zotero\\storage\\LG7F9Y4R\\Plaat - 2024 - Research Re search & Re-search.pdf;C\:\\Users\\ibyad\\Zotero\\storage\\QBNIG2GN\\2403.html}
}

@incollection{xieResearchImprovementAlphaBeta2022,
  title = {Research and {{Improvement}} of {{Alpha-Beta Search Algorithm}} in {{Gobang}}},
  author = {Xie, Yuan and Gao, Wenliang and Dai, Zuxu and Li, Yuanyuan},
  date = {2022-02-01},
  doi = {10.3233/ATDE220084},
  url = {https://www.researchgate.net/publication/367787222_Research_and_Improvement_of_Alpha-Beta_Search_Algorithm_in_Gobang},
  urldate = {2024-12-10},
  abstract = {PDF | In allusion to the Alpha-Beta search algorithm which is widely used in Gobang intelligent algorithm, this paper presents an improved method, that... | Find, read and cite all the research you need on ResearchGate},
  isbn = {978-1-64368-254-9},
  langid = {english},
  file = {C\:\\Users\\ibyad\\Zotero\\storage\\2RAQS9UW\\2024 - (PDF) Research and Improvement of Alpha-Beta Search Algorithm in Gobang.pdf;C\:\\Users\\ibyad\\Zotero\\storage\\FEKH6S4Z\\367787222_Research_and_Improvement_of_Alpha-Beta_Search_Algorithm_in_Gobang.html}
}
